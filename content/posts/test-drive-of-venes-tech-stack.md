---
title: "Test drive of VENES stack for searching open datasets -Part 1"
subtitle: "Hosted on a diverse cloud infrastructure "
date: 2017-11-15T00:58:09-05:00
categories: [Projects, Technology, Programming]
tags: ["vue", "elasticsearch","heroku","nodejs","aws","cloud","express"]
draft: false
---
### What is VENES?
**V**::Vuejs| 
**E**::Express| 
**N**::Nodejs|
**ES**::Elasticsearch
![stack](/img/venes.png)
### What we’re building
Given several open data sets or dataset apis, the idea is to extract the datasets, transform and load them into an [Elasticsearch](https://www.elastic.co/) cluster for fast searches via the Elasticsearch api. Users should be able to type in terms in a text box, google search style, and get instant relevant hits, then they should be able to narrow down their choices using some filtering mechanism. The ultimate vision is to have a more artifact focused search where users can search on a particular dataset, say recalls, and perform more focused searches.

### Why build it?
Building it out of curiosity really. I noticed a lot of text heavy datasets were being exposed as APIs by different organizations and agencies but there wasn’t a way to search across all the different apis. There a few reasons why I wanted to build a full unified search application:
The real interest was when I started evaluating [Solr vs Elasticsearch](https://logz.io/blog/solr-vs-elasticsearch/) in 2016 and was really impressed by both tools, but what the elastic team has built I thought was more than a search engine, it was a powerful and flexible platform. I’m usually skeptical of any tool that tries to do and be everything under the sun but the different elastic tools that complement elasticsearch, do their job very well. I needed to get familiar with the [Elasticsearch DSL](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html), or at least enough of it where I can build these aggregations/facets and filters using a modern web UI framework.



### The Technology Stack & Architecture
 The idea was to decouple the web front end with the server at development time so that it’s possible to fiddle with different Elasticsearch clients to see which was a better fit for the server side API. The technology behind the api that would proxy the Elasticsearch interfaces is written in NodeJS. The choice was between NodeJS and Golang Follow along and i’ll tell you why later.
The infrastructure is composed of Heroku for hosting the web app, mlab for possible user management and metrics storage (this could be Postgres, have not decided) and AWS elasticsearch OR elasticcloud, both are eventually on AWS but one is fully managed by the elastic team and one is fully managed by AWS elasticsearch service.

![infrastructure](/img/infra.png)



### Decision on programming languages and technology stacks

#### The front end
  Ahhh... the gotchas and the deceptive nature of Javascript including it's super powerful features!! . I previously had some experience with AngularJS and was excited by it’s promise but the JavaScript world never stoped evolving, never stopped moving, I carefully followed the explosion of tools and framework battles, built some basic apps with at least 4 frameworks/libraries (`Angular 2`, `Aurelia`, `VueJs`, `Knockout`) and realized that my decision would be based on the framework that gets me the less frustrated, in comes VueJs.

#### Angular2
Even tough i believe Angular 2 is a nice upgrade from AngularJS with the introduction of Typescript and i understand the goal, the framework demanded me to commit and conform too much to a particular style of building apps. If i were building this app within a large team, specially within an organisation, Angular would be my go to language for the simple reason that, The default design of the framework's api mirror the n-tier layered architecure and design patterns that developers have been using in `Java` and `C#` for a long time.
#### Aurelia 
Aurelia with typescript was very similar to Angular but it opted for present and future ECMAscript syntax. I thought it was great, if Aurelia had the support of Angular, i would chose Aurelia over Angular for building large apps with large teams within an organisation. The support is fanstastic but depending on the organisation, they tend to go with the products that are backed by the biggest companies.
#### Vue
At first i saw the `.vue` files generated by the vue-cli , all the properties, css , javascript and html in the same file, this made me cringe, i closed the project. I thought ok, why am i in such a hurry, take some time and try again. I started reading the wonderful documentation on the VueJS site, which helped a great deal, VueJS is all about keeping the cognitive mental model of an isolated component in «now memory». The VueJS components are self contained, they accept data/commands as inputs, animate and/or produce events and come alive. I hit the ground running pretty fast with Vue and never looked back. 
#### The back end APIs
I wanted a dynamic/semi dynamic or lightweight language which means I had to learn something else and that was exciting to me.
My "home" programming language is C#.NET but in order to experiment in the cloud's free tiers (no longer have anything free on Azure), i'de need to switch to dotenet core but i didn't want to go there, at least not now. Before dotnet core came out i had moved in with Golang, also, a cloud platform such as [heroku](https://www.heroku.com/) currently doesn't support dotnet core, at least not right out of the box.
The prospect of creating an all javascript css html app ...yeah that was a challenge i was looking forward to.

I chose Nodejs because well, Javascript… I really started with Golang but the Go elasticsearch client is not yet mature, although it’s highly performant and is constatntly improving. In Node, the elasticsearch javascript client seems a natural fit given the Restful nature of the elasticsearch API. Here is an example of what a elasticsearch query looks like from the elasticsearch Restful API :

{{< highlight json "linenos=inline">}}

{
    "query": {
        "bool": {
            "must": [

                {
                    "match": {
                        "_all": "fire"
                    }
                }
            ],

            "filter": {
                "bool": {
                    "must": [{
                            "terms": {
                                "_type": ["neissreport"]
                            }

                        },

                        {
                            "range": {
                                "artifactDate": {
                                    "gte": "1970-09-20",
                                    "lte": "2009-09-26"
                                }
                            }
                        }
                    ]

                }
            }

        }

    },
    "sort": [
        { "_type": { "order": "desc" } },
        { "artifactDate": { "order": "desc" } }

    ],
    "aggregations": {
        "artifact_type": {
            "terms": {
                "field": "type.keyword"
            }
        },
        "artifact_source": {
            "terms": {
                "field": "artifactSource.keyword"
            }
        }
    }


}

{{</ highlight >}}

This elasticsearch query performs a search where the word *fire* is present, the `match all` directive tells elasticsearch to looks for *fire* in every field in the document that has text data and then perform a filter on the result where the only data that gets return are of type "neissreport" AND are between 1970 and 2009. The data is then sorted by type and artifact date. Then an aggregation is also returned, it retrieves the number of indexed documents that meet all the conditions of the aggregation (grouping)
If you are familiar with `SQL` then this would sort of be :
```sql
SELECT [fields] FROM index_name 
WHERE fulltext_field LIKE '%fire%'
AND _type = 'neissreport'
AND artifactDate BETWEEN '1970-09-20' AND '2009-09-26'
GROUP BY artifact_Source, artifact_type
ORDER BY _type, artifactDate DESC
```
The only difference is that, this query would not run in the SQL engine because you have to specify the fields that are going to be grouped and you can really mix aggregation queries with resultset queries in plain SQL. You'de have to write a stored procedure.

In part 2 or 3 of this blog, i will talk about using the [bodybuilder](http://bodybuilder.js.org/) javascript package that reduces the size of the json query and makes it more legible, like so :
```javascript
bodybuilder()
        .query('match', '_all', 'fire')
        .andFilter('terms', '_type', 'neissreport')
        .andFilter('range', 'artifactDate', [{ to: '2009-09-26' }, { from: '1970-09-20' }])
        .sort('_type','desc')
        .sort('artifactDate','desc')
        .aggregation('terms', 'type.keyword')
        .aggregation('terms','artifactSource.keyword')
        .build()
```
A quick and simple example of how the UI populating the query would look like : https://codepen.io/bizoton19/pen/VraadN


